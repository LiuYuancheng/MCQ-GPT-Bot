{% extends 'mgmbase.html' %}

{% block title %} User Action scheduler Monitor {% endblock %}

{% block style%}
<style>
  /* Make the image fully responsive 
  .carousel-inner img {
    width: 100%;
    height: 50%;
  }*/
</style>

{% endblock %}

{% block mgmContent %}

<ul class="list-group">

  <li class="list-group-item">
    <h3> MCQ-GPT-Bot </h3>
    <p> We want to create an assistant AI-Bot program which can batch process the multi 
      choice cyber security exam questions (From different source format : md, url, html, 
      txt, pdf, json) via OpenAI to get the answers so the researcher can use it to check 
      the AI's answer correctness rate, AI's performance on solving question on different 
      fields and do further data analysis. </p>
  </li>

  <li class="list-group-item">
    <h4> <span class="badge badge-secondary">Introduction </span> </h4>
    <p> <b> The MCQ-GPT-Bot will is an automate AI-Bot assistant program which provides below functions: </b>
    </p>

    <p><b> &#149; Parse multi-choice-questions from different format data source to build the standard question bank files for the further process such as training (data normalization) . </b></p>
    <p><b> &#149; If the question sources don't content the answer, use OpenAI to get the answer (with or without the scenario prompt) .  </b></p>
    <p><b> &#149; If the question sources also provide the answer, compare with AI's answer and calculate the AI's correctness rate.</b></p>
    <p>The program will use the LLM LangChain frame work to implement the communication with the OpenAI-API.  We will also provide a web UI (under development) for the researcher to test their prompt or normal user who want to do quick test.</p>
  </li>


  <li class="list-group-item">

    <div class="row">
      <div class="col-sm-8">
        <h4> <span class="badge badge-secondary"> System Structure </span> </h4>
        <p><b> The program is a single thread program to continuous loading all the question 
          source files/urls set in the config file, convert to the standard question format, 
          then based on the question type setup the LLM's scenario prompt and send to the 
          questions solver to get the AI's solution. If you have multiple OpenAI-API, you can 
          also config multi-thread with several parser and question solver to increase the 
          processing efficiency.</b></p>

        <dl>
          <dt>QuestionParser</dt>
          <dd> &#149; The MCQ question data parser (QuestionParser) will do the data mining and 
            normalization step, it will load all the contents from MCQ source files or URL, 
            use AI to find all the MCQs, then generate the standard question bank data.</dd>

          <dt>McqDataManager</dt>
          <dd> &#149; The MCA data manager (McqDataManger) will do the data storage and result 
            archiving step, it will store and questions, AI's answers and format the result. 
            As we will batch process multiple question source, the data manager will log the 
            process progress (such as whether a source file's result has been archived) so if 
            there is program execution interruption happens, when the user run again, the don't 
            need to process the source from the beginning. </dd>

          <dt> llmMcqSolver  </dt>
          <dd> &#149; The large language MCQ solver (llmMcqSolver) will fetch the question 
            from the data manager, preload the MCQ questions scenario prompt to OpenAI, then 
            call OpenAI API to get the answer and calculate the AI's correctness rate based on 
            the setting. </dd>
        </dl>
      </div>
      <div class="col-sm-4">
        <img src="{{url_for('static', filename='img/workflow.png')}}" align="left" class="img-thumbnail" />

      </div>
    </div>

  </li>

  <li class="list-group-item">
    <h4> <span class="badge badge-secondary"> Current version : </span>
      <a type="button" class="btn btn-outline-secondary btn-sm"
        href="https://github.com/LiuYuancheng/MCQ-GPT-Bot" target="_blank">
        <span class="badge badge-light"> v_0.1.2</span>
        <span class="spinner-grow spinner-grow-sm"> </span> [Project Github link]
      </a>
    </h4>
  </li>
</ul>
{% endblock %}